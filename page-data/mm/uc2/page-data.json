{"componentChunkName":"component---src-pages-mm-uc-2-index-mdx","path":"/mm/uc2/","result":{"pageContext":{"frontmatter":{"title":"Use Case 2 - Flat line causing Slow Response Time","description":null},"relativePagePath":"/mm/uc2/index.mdx","titleType":"page","MdxNode":{"id":"33389486-bf7d-52dc-ba70-40962ed3f2c4","children":[],"parent":"4513f0b3-c003-5322-931b-b8a0763ef172","internal":{"content":"---\ntitle: Use Case 2 - Flat line causing Slow Response Time\ndescription: \n---\n\n## Metric Manager Anomaly events.\n\n  The usual way that an Operation center works is through `Management by Exception`. Basically, the SREs are expected to be notified if something goes wrong.  Metric Manager improves this by sending anomalies.  It sends events when something is about to go wrong.  These events allow the SREs with `Incident Avoidance` tools, hence improving `Meantime between failure`.\n\n## Flatline causing Slow Response Time.\n\n  When a metric typically varies and then stops varying, it almost always means something has gone wrong. It's stuck, or something feeding it is stuck, or something it's sending to is stuck. The ability to detect this behavior is beneficial. All customers with systems with limits of some kind – memory on a machine, connections to a database, traffic through a link, flow through a pipe, etc, will find this valuable. Metric Manager allows you to detect the threshold has been met without knowing the actual limit itself.\n\n## The Scenario.\n\n  The data for this use case was from a customer when the connections to their database were behaving unexpectedly. The connection pool for the customer's application normally grew and shrunk constantly throughout the day as the usage of the system varied. Still, Predictive Insights detected that this behavior had changed. The application was stuck at the limit of its resources, which slowed the performance for users. It gave a clear starting point and root cause for a poor performance issue, which is notoriously difficult to diagnose.\n\n### The lab exercise.\n\n  Close your current tab from the previous use case.   Click the Flag icon (for incidents) from the menu strip on the right.  Select the `Detected Anomalies` menu item.  If you are not familiar with Watson AIOps Event Viewer, this is the `Event Viewer` page with the `MetricManagerAnomalies` Filter and `MetricManagerView` View already preselected.\n\n  <img src=\"./images/UC20.png\" alt=\"Watson AIOps Metric Manager\" width=\"900\" align=\"center\"/>\n\n  Observe:\n  - From the top bar, you can see that there is 1 `Major` events (Orange Exclamation Symbol) and 12 `Minor` events (Yellow Exclamation Symbol).\n  - The default Severity for the anomaly events is `Minor`.\n  - Check the list for individual anomalies discovered by Predictive Insights. You can see when an anomaly has started and finished occurring in the `FirstOccurrence` and `LastOccurrence` columns. `Summary` column describes the anomaly. `Count` shows how many times the anomaly has occurred. Columns like `Node`, `AnomalousResource`, `AnomalousMetric`, and `Direction` provide additional information on the anomaly.\n  - There is one event with the severity `Major`. The major anomalies represent consolidated events found by Metric Manager. In the `Summary` column, you can see the type of consolidation and how many metrics or nodes are involved.\n\n  Click the event with Node value `PrimaryKWPUBIIS06`, and Summary `Connectionpoolsize is now a flat line where before it was varying`.\n\n  Observe:\n\n  - The metric affected is `Connectionpoolsize`. The `summary` tells you that this metric is flat. The direction column says it is `Higher` than expected. The odds are there is so much usage a ceiling of some kind has been hit. If the direction were `Lower`, we would suspect that usage had dropped right off and that the service connecting to this was stopped or broken.\n\n  Right-click on the event, and select __ServiceDiagnosis__ from the pop-up menu.\n\n  <img src=\"./images/UC21.png\" alt=\"Watson AIOps Metric Manager\" width=\"900\" align=\"center\"/>\n\n  You will see the following graph:\n\n  <img src=\"./images/UC22.png\" alt=\"Watson AIOps Metric Manager\" width=\"900\" align=\"center\"/>\n\n  Observe:\n\n  - The behavior of `Connectionpoolsize` for the last week can be seen.\n  - The green area is the baseline that indicates the expected range of values. \n  - The red zone at the right of the chart shows where something unexpected has happened. Here Metric Manager raises an anomaly while the metric value is __inside__ the baseline. Metric Manager does this because the metric normally varies in value, but it has now gone completely flat. The monitored system has hit a limit in the maximum number of connections.\n\n  We want to zoom on the diagram.  So click on the start diagram anywhere you want to see the start time and pull to the right until you passed the red box.  See the following chart for approximate zoom area:\n\n  <img src=\"./images/UC23.png\" alt=\"Watson AIOps Metric Manager\" width=\"600\" align=\"center\"/>\n\n  Observe:\n\n  - In the zoomed graph, hover your mouse on top of the flat line, and you can see that the metrics have hit the 3,200 `connection pool size`.\n  - Somewhere in this system, the limit for this connection pool has been set to 3,200. The system has slowed down because all the connections are in use, so users have to wait for connections to be freed before responding. Enough requests have built up so that they are immediately consumed as soon as any connections are released.\n\n  <img src=\"./images/UC24.png\" alt=\"Watson AIOps Metric Manager\" width=\"600\" align=\"center\"/>\n\n  Look at the bottom part of the page, and you can see the causal group.\n  Note the `Processortime` and the `Usertime` are in the causal group, with a flat line anomaly.\n  Select the `Usertime` metrics.\n\n  <img src=\"./images/UC25.png\" alt=\"Watson AIOps Metric Manager\" width=\"900\" align=\"center\"/>\n\n  Observe:\n\n  - When the `Connectionpoolsize` hits a limit of 3,200, the User processor time becomes starved. This translates to **Slow Response Time**.  The metric Response time, in this case, is not provided to Metric Manager. However, you can conclude that as the system has reached its maximum connection pool size, the service and users that depend on the connection have to wait for the connection to be released; this translates to poor performance.\n\n  Summary during the customer interaction on this use case: \n\n  In typical operation, the system runs close to its limit. That is why the system slows down intermittently and why these slowdowns are hard to diagnose. Only a tiny increase in average load is required to put the system under pressure. Traditional monitoring will struggle to detect these cases because you would need to find and then apply the exact connection pool configuration for each occurrence of this metric (that's going to be time-consuming and expensive). Then you will need to remember to update this threshold every time an application change is made. Predictive Insights finds this case automatically across every single metric. So you are automatically covered for connection pool size, memory, processor usage, and many others without any configuration.\n\n### Customer Quotes\n\n  - \"_Before Predictive Insights (previous name of Metric Manager), it took 30 minutes to decide what to investigate. Now it takes less than 3 minutes._\"\n","type":"Mdx","contentDigest":"a79b479e8d0098837b366561ef42790d","counter":158,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Use Case 2 - Flat line causing Slow Response Time","description":null},"exports":{},"rawBody":"---\ntitle: Use Case 2 - Flat line causing Slow Response Time\ndescription: \n---\n\n## Metric Manager Anomaly events.\n\n  The usual way that an Operation center works is through `Management by Exception`. Basically, the SREs are expected to be notified if something goes wrong.  Metric Manager improves this by sending anomalies.  It sends events when something is about to go wrong.  These events allow the SREs with `Incident Avoidance` tools, hence improving `Meantime between failure`.\n\n## Flatline causing Slow Response Time.\n\n  When a metric typically varies and then stops varying, it almost always means something has gone wrong. It's stuck, or something feeding it is stuck, or something it's sending to is stuck. The ability to detect this behavior is beneficial. All customers with systems with limits of some kind – memory on a machine, connections to a database, traffic through a link, flow through a pipe, etc, will find this valuable. Metric Manager allows you to detect the threshold has been met without knowing the actual limit itself.\n\n## The Scenario.\n\n  The data for this use case was from a customer when the connections to their database were behaving unexpectedly. The connection pool for the customer's application normally grew and shrunk constantly throughout the day as the usage of the system varied. Still, Predictive Insights detected that this behavior had changed. The application was stuck at the limit of its resources, which slowed the performance for users. It gave a clear starting point and root cause for a poor performance issue, which is notoriously difficult to diagnose.\n\n### The lab exercise.\n\n  Close your current tab from the previous use case.   Click the Flag icon (for incidents) from the menu strip on the right.  Select the `Detected Anomalies` menu item.  If you are not familiar with Watson AIOps Event Viewer, this is the `Event Viewer` page with the `MetricManagerAnomalies` Filter and `MetricManagerView` View already preselected.\n\n  <img src=\"./images/UC20.png\" alt=\"Watson AIOps Metric Manager\" width=\"900\" align=\"center\"/>\n\n  Observe:\n  - From the top bar, you can see that there is 1 `Major` events (Orange Exclamation Symbol) and 12 `Minor` events (Yellow Exclamation Symbol).\n  - The default Severity for the anomaly events is `Minor`.\n  - Check the list for individual anomalies discovered by Predictive Insights. You can see when an anomaly has started and finished occurring in the `FirstOccurrence` and `LastOccurrence` columns. `Summary` column describes the anomaly. `Count` shows how many times the anomaly has occurred. Columns like `Node`, `AnomalousResource`, `AnomalousMetric`, and `Direction` provide additional information on the anomaly.\n  - There is one event with the severity `Major`. The major anomalies represent consolidated events found by Metric Manager. In the `Summary` column, you can see the type of consolidation and how many metrics or nodes are involved.\n\n  Click the event with Node value `PrimaryKWPUBIIS06`, and Summary `Connectionpoolsize is now a flat line where before it was varying`.\n\n  Observe:\n\n  - The metric affected is `Connectionpoolsize`. The `summary` tells you that this metric is flat. The direction column says it is `Higher` than expected. The odds are there is so much usage a ceiling of some kind has been hit. If the direction were `Lower`, we would suspect that usage had dropped right off and that the service connecting to this was stopped or broken.\n\n  Right-click on the event, and select __ServiceDiagnosis__ from the pop-up menu.\n\n  <img src=\"./images/UC21.png\" alt=\"Watson AIOps Metric Manager\" width=\"900\" align=\"center\"/>\n\n  You will see the following graph:\n\n  <img src=\"./images/UC22.png\" alt=\"Watson AIOps Metric Manager\" width=\"900\" align=\"center\"/>\n\n  Observe:\n\n  - The behavior of `Connectionpoolsize` for the last week can be seen.\n  - The green area is the baseline that indicates the expected range of values. \n  - The red zone at the right of the chart shows where something unexpected has happened. Here Metric Manager raises an anomaly while the metric value is __inside__ the baseline. Metric Manager does this because the metric normally varies in value, but it has now gone completely flat. The monitored system has hit a limit in the maximum number of connections.\n\n  We want to zoom on the diagram.  So click on the start diagram anywhere you want to see the start time and pull to the right until you passed the red box.  See the following chart for approximate zoom area:\n\n  <img src=\"./images/UC23.png\" alt=\"Watson AIOps Metric Manager\" width=\"600\" align=\"center\"/>\n\n  Observe:\n\n  - In the zoomed graph, hover your mouse on top of the flat line, and you can see that the metrics have hit the 3,200 `connection pool size`.\n  - Somewhere in this system, the limit for this connection pool has been set to 3,200. The system has slowed down because all the connections are in use, so users have to wait for connections to be freed before responding. Enough requests have built up so that they are immediately consumed as soon as any connections are released.\n\n  <img src=\"./images/UC24.png\" alt=\"Watson AIOps Metric Manager\" width=\"600\" align=\"center\"/>\n\n  Look at the bottom part of the page, and you can see the causal group.\n  Note the `Processortime` and the `Usertime` are in the causal group, with a flat line anomaly.\n  Select the `Usertime` metrics.\n\n  <img src=\"./images/UC25.png\" alt=\"Watson AIOps Metric Manager\" width=\"900\" align=\"center\"/>\n\n  Observe:\n\n  - When the `Connectionpoolsize` hits a limit of 3,200, the User processor time becomes starved. This translates to **Slow Response Time**.  The metric Response time, in this case, is not provided to Metric Manager. However, you can conclude that as the system has reached its maximum connection pool size, the service and users that depend on the connection have to wait for the connection to be released; this translates to poor performance.\n\n  Summary during the customer interaction on this use case: \n\n  In typical operation, the system runs close to its limit. That is why the system slows down intermittently and why these slowdowns are hard to diagnose. Only a tiny increase in average load is required to put the system under pressure. Traditional monitoring will struggle to detect these cases because you would need to find and then apply the exact connection pool configuration for each occurrence of this metric (that's going to be time-consuming and expensive). Then you will need to remember to update this threshold every time an application change is made. Predictive Insights finds this case automatically across every single metric. So you are automatically covered for connection pool size, memory, processor usage, and many others without any configuration.\n\n### Customer Quotes\n\n  - \"_Before Predictive Insights (previous name of Metric Manager), it took 30 minutes to decide what to investigate. Now it takes less than 3 minutes._\"\n","fileAbsolutePath":"/Users/dymaczew/temp/cp4wa-lab/src/pages/mm/uc2/index.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","3018647132","3037994772","768070550"]}